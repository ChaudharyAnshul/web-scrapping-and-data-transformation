{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aba8059b-ac9c-4b85-86e4-9fdcf2cd2814",
   "metadata": {},
   "source": [
    "# PDF Data Extraction: PyPDF2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24e958b6-ce97-4b38-ac9c-37022a7d03b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import PyPDF2\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1fd4c0b-f104-45a0-bf54-f0c9429d4977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to ead and process extracted data from PDFs using PyPDF2\n",
    "def process_pdf(input_pdf_path, output_txt_path):\n",
    "    pdfFileObj = open(input_pdf_path, 'rb')\n",
    "    pdfReader = PyPDF2.PdfReader(pdfFileObj)\n",
    "\n",
    "    print(f\"Processing {input_pdf_path}\")\n",
    "    print(\"Total number of pages:\", len(pdfReader.pages))\n",
    "\n",
    "    # Extracting level and year information\n",
    "    level, year = extract_information(input_pdf_path)\n",
    "\n",
    "    # Checking if level and year information is available\n",
    "    if level and year:\n",
    "        output_txt_path = f\"..\\\\..\\\\data\\\\extracted-pdf-data_PyPDF2\\\\PyPDF_RR__{year}_level{level}_combined.txt\"\n",
    "\n",
    "    else:\n",
    "        print(f\"Level and year information not found for {input_pdf_path}. Skipping...\")\n",
    "        return\n",
    "\n",
    "    # Initializing dictionaries to store extracted content\n",
    "    content = dict()\n",
    "    topic = \"\"\n",
    "    topic_dict = dict()\n",
    "\n",
    "    # Iterating through each page of the PDF and extracting text\n",
    "    for page_num in range(len(pdfReader.pages)):\n",
    "      t = pdfReader.pages[page_num].extract_text().split('\\n')\n",
    "      line_num = 0\n",
    "\n",
    "      # Extracting topic names\n",
    "      while line_num < len(t):\n",
    "          if line_num == 0:\n",
    "              if 'topic outlines' in t[line_num].strip().lower():\n",
    "                  line_num += 1\n",
    "              topic_new = re.sub(r'[^A-Za-z ]+', '', t[line_num]).strip()\n",
    "\n",
    "              # Checking if the topic already exists in the content dictionary\n",
    "              all_keys = [x.lower().strip().replace(\" \", \"\") for x in content.keys()]\n",
    "              if topic_new.lower().strip().replace(\" \", \"\") in all_keys:\n",
    "                  topic_new = list(filter(lambda x: x.lower().strip().replace(\" \", \"\") == topic_new.lower().strip().replace(\" \", \"\"), content.keys()))[0]\n",
    "\n",
    "              # Updating the topic if it has changed\n",
    "              if topic == topic_new:\n",
    "                  pass\n",
    "              else:\n",
    "                  subtopic = \"\"\n",
    "                  subtopic_dict = []\n",
    "                  topic = topic_new\n",
    "          topic_dict = content.get(topic, dict())\n",
    "\n",
    "          # Identifying subtopics i.e. Learning outcoomes for Topics\n",
    "          subtopic_loc = t[line_num].find(\"The candidate should be able to:\")\n",
    "          if subtopic_loc != -1:\n",
    "              subtopic = t[line_num - 1] if subtopic_loc == 0 else t[line_num][:subtopic_loc + 1]\n",
    "              subtopic_dict = topic_dict.get(subtopic, [])\n",
    "              tab_loc = t[line_num].find(\"\\t\")\n",
    "\n",
    "              # Extracting learning outcomes and appending to the subtopic dictionary\n",
    "              append_list = t[line_num][tab_loc + 1:] + t[line_num + 1]\n",
    "              if append_list.find(\"\\t\") == -1:\n",
    "                  subtopic_dict.append(append_list)\n",
    "\n",
    "                  line_num += 2\n",
    "              if line_num >= len(t):\n",
    "                  break\n",
    "          # Handeling corner cases, extract learning outcomes from lines with tabs\n",
    "          tab_loc = t[line_num].find(\"\\t\")\n",
    "\n",
    "          if tab_loc != -1:\n",
    "              subtopic_dict.append(t[line_num][tab_loc + 1:])\n",
    "\n",
    "          # Updating the topic dictionary\n",
    "          topic_dict[subtopic] = subtopic_dict\n",
    "          content[topic] = topic_dict\n",
    "\n",
    "          line_num += 1\n",
    "\n",
    "    # Writing the extracted data to an output text file\n",
    "    with open(output_txt_path, 'w', encoding='utf-8') as output_file:\n",
    "        for topic, subtopics in content.items():\n",
    "            output_file.write(f\"\\nTopic: {topic}\\n\")\n",
    "            output_file.write(\"\\t\\n\\tLearning Outcomes: \\n\")\n",
    "            output_file.write(\"\\t\\t(For the below Learning Outcomes, The candidate should be able to: )\\n\")\n",
    "            for subtopic, learning_outcomes in subtopics.items():\n",
    "                if subtopic == '':\n",
    "                    continue\n",
    "                output_file.write(f\"\\t{subtopic}\\n\")\n",
    "                for outcome in learning_outcomes:\n",
    "                    output_file.write(f\"\\t\\t- {outcome}\\n\")\n",
    "\n",
    "    print(f\"Output saved to: {output_txt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "203320e6-f864-4221-99b2-99af97dcf737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract year and level, since the code is returning output files as per givrn naming convention dynamically\n",
    "def extract_information(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "        # Getting the text from the first page\n",
    "        first_page_text = pdf_reader.pages[0].extract_text()\n",
    "\n",
    "        # Using regular expression to find the pattern to extract Year and Level\n",
    "        pattern = r'(\\d{4})\\s*Level\\s*((\\w)+)'\n",
    "        match = re.search(pattern, first_page_text)\n",
    "\n",
    "        if match:\n",
    "            year = match.group(1)\n",
    "            level = match.group(2)\n",
    "            return level, year\n",
    "        else:\n",
    "            return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aa3a4c0-3218-48d0-bb6d-0e7e99a47b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Giving input PDFs\n",
    "pdf_files = ['..\\\\..\\\\data\\\\raw-pdf-data\\\\2024-l1-topics-combined-2.pdf', \n",
    "             '..\\\\..\\\\data\\\\raw-pdf-data\\\\2024-l2-topics-combined-2.pdf', \n",
    "             '..\\\\..\\\\data\\\\raw-pdf-data\\\\2024-l3-topics-combined-2.pdf']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebd9ec04-6806-4bc3-9b45-1e1e0412e541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ..\\..\\data\\raw-pdf-data\\2024-l1-topics-combined-2.pdf\n",
      "Total number of pages: 27\n",
      "Output saved to: ..\\..\\data\\extracted-pdf-data_PyPDF2\\PyPDF_RR__2024_levelI_combined.txt\n",
      "Processing ..\\..\\data\\raw-pdf-data\\2024-l2-topics-combined-2.pdf\n",
      "Total number of pages: 25\n",
      "Output saved to: ..\\..\\data\\extracted-pdf-data_PyPDF2\\PyPDF_RR__2024_levelII_combined.txt\n",
      "Processing ..\\..\\data\\raw-pdf-data\\2024-l3-topics-combined-2.pdf\n",
      "Total number of pages: 18\n",
      "Output saved to: ..\\..\\data\\extracted-pdf-data_PyPDF2\\PyPDF_RR__2024_levelIII_combined.txt\n"
     ]
    }
   ],
   "source": [
    "# Mapping output\n",
    "for pdf_file in pdf_files:\n",
    "    process_pdf(pdf_file, output_txt_path='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2f6545-2c2d-40c8-8967-c47d8448a518",
   "metadata": {},
   "source": [
    "### Extracted data has been saved successfully @..\\\\..\\\\data\\\\extracted-pdf-data_PyPDF2\\\\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df512422",
   "metadata": {},
   "source": [
    "# PDF Data Extraction: Grobid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "182c16f3-b0bf-4d9a-8da3-e92b5ad59883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grobid_client_python.grobid_client.grobid_client import GrobidClient\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15873e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdfs(input_directory, output_directory):\n",
    "    client = GrobidClient(config_path=\"./config.json\")\n",
    "    client.process(\"processFulltextDocument\", input_directory, output_directory, n=1, consolidate_header=False,consolidate_citations=True,force=True,segment_sentences=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b8d6bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_to_text(xml_string):\n",
    "    root = ET.fromstring(xml_string)\n",
    "    text = \"\"\n",
    "    for elem in root.iter():\n",
    "        if elem.text:\n",
    "            text += elem.text + \"\\n\"\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1ea7fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROBID server is up and running\n",
      "../../data/extracted-pdf-data_Grobid/2024-l2-topics-combined-2.grobid.tei.xml\n",
      "Converted 2024-l2-topics-combined-2.grobid.tei.xml to Grobid_RR_2024_2_combined.txt\n",
      "../../data/extracted-pdf-data_Grobid/2024-l1-topics-combined-2.grobid.tei.xml\n",
      "Converted 2024-l1-topics-combined-2.grobid.tei.xml to Grobid_RR_2024_1_combined.txt\n",
      "../../data/extracted-pdf-data_Grobid/2024-l3-topics-combined-2.grobid.tei.xml\n",
      "Converted 2024-l3-topics-combined-2.grobid.tei.xml to Grobid_RR_2024_3_combined.txt\n"
     ]
    }
   ],
   "source": [
    "input_directory = \"../../data/raw-pdf-data\"  \n",
    "output_directory = \"../../data/extracted-pdf-data_Grobid/\"\n",
    "\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "process_pdfs(input_directory, output_directory)\n",
    "pattern = re.compile(r\"2024-l(\\d+)-\")\n",
    "for filename in os.listdir(output_directory):\n",
    "    if filename.endswith(\".xml\"):\n",
    "        match = pattern.search(filename)\n",
    "        if match:\n",
    "            extracted_number = int(match.group(1))\n",
    "            xml_file_path = os.path.join(output_directory, filename)\n",
    "            print(xml_file_path)\n",
    "            with open(xml_file_path, \"r\", encoding=\"utf-8\") as xml_file:\n",
    "                xml_content = xml_file.read()\n",
    "                text_content = xml_to_text(xml_content)\n",
    "                txt_file_path = output_directory + \"Grobid_RR_\" + str(datetime.date.today().year) + \"_\" +  str(extracted_number)+ \"_combined\"+ \".txt\"\n",
    "                with open(txt_file_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
    "                    txt_file.write(text_content)\n",
    "                print(f\"Converted {filename} to {os.path.basename(txt_file_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ed3178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory containing XML files\n",
    "xml_directory = '../../data/extracted-pdf-data_Grobid'\n",
    "\n",
    "# Create a CSV file for storing metadata\n",
    "csv_file_path = '../../data/extracted-pdf-data_Grobid/grobid_metadata.csv'\n",
    "if os.path.exists(csv_file_path):\n",
    "    os.remove(csv_file_path)\n",
    "    \n",
    "csv_file = open(csv_file_path, 'w', newline='', encoding='utf-8')\n",
    "\n",
    "csv_writer = csv.writer(csv_file)\n",
    "\n",
    "# Write header row to CSV file\n",
    "csv_writer.writerow(['Filename', 'Time','MD5 Identifier', 'Encoding Version','Lang', 'Application Identifier',\n",
    "                     'Application Description', 'Application Version', 'Application Reference URL'])\n",
    "\n",
    "\n",
    "# Iterate through XML files in the directory\n",
    "for filename in os.listdir(xml_directory):\n",
    "    if filename.endswith('.xml'):\n",
    "        # Parse the XML document\n",
    "        tree = ET.parse(os.path.join(xml_directory, filename))\n",
    "        root = tree.getroot()\n",
    "\n",
    "        when_attribute = root.find('.//{http://www.tei-c.org/ns/1.0}appInfo/{http://www.tei-c.org/ns/1.0}application[@ident=\"GROBID\"]')\n",
    "        when_attribute = when_attribute.get('when') if when_attribute is not None else \"\"\n",
    "\n",
    "        md5_identifier = root.find('.//{http://www.tei-c.org/ns/1.0}sourceDesc/{http://www.tei-c.org/ns/1.0}biblStruct/{http://www.tei-c.org/ns/1.0}idno[@type=\"MD5\"]')\n",
    "        md5_identifier = md5_identifier.text if md5_identifier is not None else \"\"\n",
    "\n",
    "        version_attribute = root.get('encoding') if 'encoding' in root.attrib else \"UTF-8\"\n",
    "\n",
    "        lang_attribute = root.get('lang') if 'lang' in root.attrib else \"en\"\n",
    "\n",
    "        application_identifier = root.find('.//{http://www.tei-c.org/ns/1.0}appInfo/{http://www.tei-c.org/ns/1.0}application[@ident=\"GROBID\"]')\n",
    "        application_identifier = application_identifier.get('ident') if application_identifier is not None else \"\"\n",
    "\n",
    "        application_description = root.find('.//{http://www.tei-c.org/ns/1.0}appInfo/{http://www.tei-c.org/ns/1.0}application[@ident=\"GROBID\"]/{http://www.tei-c.org/ns/1.0}desc')\n",
    "        application_description = application_description.text if application_description is not None else \"\"\n",
    "\n",
    "        application_version = root.find('.//{http://www.tei-c.org/ns/1.0}appInfo/{http://www.tei-c.org/ns/1.0}application[@ident=\"GROBID\"]')\n",
    "        application_version = application_version.get('version') if application_version is not None else \"\"\n",
    "\n",
    "        application_reference_url = root.find('.//{http://www.tei-c.org/ns/1.0}appInfo/{http://www.tei-c.org/ns/1.0}application[@ident=\"GROBID\"]/{http://www.tei-c.org/ns/1.0}ref')\n",
    "        application_reference_url = application_reference_url.get('target') if application_reference_url is not None else \"\"\n",
    "\n",
    "        # Write metadata to CSV file\n",
    "        csv_writer.writerow([filename, when_attribute, md5_identifier, version_attribute, lang_attribute, application_identifier,\n",
    "                             application_description, application_version, application_reference_url])\n",
    "\n",
    "# Close CSV file\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa20569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25922caa-b997-484b-bad0-c9d58c168b55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
