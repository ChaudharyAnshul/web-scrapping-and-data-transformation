{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26b73e6a-2bac-4a94-9545-279c36d44a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from sqlalchemy import MetaData, Table, Column, String, Integer\n",
    "from snowflake_helper import getSnowflakeEngine\n",
    "from snowflake.connector import connect\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7fe231f-e80e-42ea-8abe-ef15c9b7b2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to upload files to S3\n",
    "def uploadFiletoS3(file_name, bucket, object_name):\n",
    "    try:  \n",
    "        # get S3 object\n",
    "        s3 = boto3.client('s3')\n",
    "\n",
    "        # upload file\n",
    "        with open(file_name, \"rb\") as f:\n",
    "             s3.upload_fileobj(f, bucket, object_name)\n",
    "    except ClientError as e:\n",
    "        print(\"Error in S3 Upload\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49ea11b5-01e2-4e3e-bb0f-a92e013df07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to upload csv and log file for web scrapping to S3\n",
    "def upload_scrape_file():\n",
    "    # initialize source locations\n",
    "    data_source = \"..\\..\\data\\scrape-data\\cfa-data.csv\"\n",
    "    log_source = \"..\\..\\logs\\scrape-log\\webscrapping.log\"\n",
    "    snowflake_source = \"..\\..\\logs\\data-load-log\\snowflake-upload.log\"\n",
    "\n",
    "    # S3 bucket name\n",
    "    bucket = \"cfa-data-t2\"\n",
    "    \n",
    "    # initialize destination locations\n",
    "    data_destination = \"csv-files/cfa-data.csv\"\n",
    "    log_destination = \"log-files/webscrapping.log\"\n",
    "    snowflake_destination = \"log-files/snowflake-upload.log\"\n",
    "\n",
    "    # upload csv file\n",
    "    uploadFiletoS3(data_source, bucket, data_destination)\n",
    "\n",
    "    # upload log file\n",
    "    uploadFiletoS3(log_source, bucket, log_destination)\n",
    "\n",
    "    # upload log file\n",
    "    uploadFiletoS3(snowflake_source, bucket, snowflake_destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "774bc166-c1cf-4cc1-8a9f-68f7824d707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to upload files to upload text file for PDF data extraction using PyPDF2 and Grobid to S3\n",
    "def upload_extracted_data_files():\n",
    "\n",
    "    # List of file paths for PyPDF2 and Grobid\n",
    "    pyPDF2_files = [\n",
    "        {\"path\": \"..\\\\..\\\\data\\\\extracted-pdf-data_PyPDF2\\\\PyPDF_RR__2024_levelI_combined.txt\", \"destination\": \"pypdf2-files/\"},\n",
    "        {\"path\": \"..\\\\..\\\\data\\\\extracted-pdf-data_PyPDF2\\\\PyPDF_RR__2024_levelII_combined.txt\", \"destination\": \"pypdf2-files/\"},\n",
    "        {\"path\": \"..\\\\..\\\\data\\\\extracted-pdf-data_PyPDF2\\\\PyPDF_RR__2024_levelIII_combined.txt\", \"destination\": \"pypdf2-files/\"},\n",
    "    ]\n",
    "    \n",
    "    grobid_files = [\n",
    "        {\"path\": \"..\\\\..\\\\data\\\\extracted-pdf-data_Grobid\\\\Grobid_RR_2024_2_combined.txt\", \"destination\": \"grobid-files/\"},\n",
    "        {\"path\": \"..\\\\..\\\\data\\\\extracted-pdf-data_Grobid\\\\Grobid_RR_2024_1_combined.txt\", \"destination\": \"grobid-files/\"},\n",
    "        {\"path\": \"..\\\\..\\\\data\\\\extracted-pdf-data_Grobid\\\\Grobid_RR_2024_3_combined.txt\", \"destination\": \"grobid-files/\"},\n",
    "    ]\n",
    "\n",
    "    \n",
    "    # S3 bucket name\n",
    "    bucket = \"cfa-data-t2\"\n",
    "    \n",
    "    # Uploading extracted data files for PyPDF2\n",
    "    for file_info in pyPDF2_files:\n",
    "        uploadFiletoS3(file_info[\"path\"], bucket, file_info[\"destination\"] + file_info[\"path\"].split(\"\\\\\")[-1])\n",
    "    \n",
    "    # Uploading extracted data files for Grobid\n",
    "    for file_info in grobid_files:\n",
    "        uploadFiletoS3(file_info[\"path\"], bucket, file_info[\"destination\"] + file_info[\"path\"].split(\"\\\\\")[-1])\n",
    "\n",
    "    S3_urls = []\n",
    "    for file_info in grobid_files:\n",
    "        S3_urls.append(\"https://{}.s3.amazonaws.com/Grobid/{}\".format(bucket, file_info[\"path\"].split(\"\\\\\")[-1]))\n",
    "    \n",
    "    return S3_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cc9f0769-cdc7-4380-a67f-b15004a79b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update S3 url in csv\n",
    "def UpdateCSV(s3_urls):\n",
    "    df = pd.read_csv(\"../../data/extracted-pdf-data_Grobid/grobid_metadata.csv\")\n",
    "    df[\"S3 URL\"] = s3_urls\n",
    "    \n",
    "    csv_location = \"..\\..\\data\\extracted-pdf-data_Grobid\\grobid_metadata_new.csv\"\n",
    "    df.to_csv(csv_location, index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bcf4b4b1-616b-4fd2-a8ad-0a920541b3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create metdata table\n",
    "def createMetadataTable(engine):\n",
    "    try:\n",
    "        print('-------Starting Snowflake table creation-------')\n",
    "        # Define metadata\n",
    "        metadata = MetaData()\n",
    "    \n",
    "        # Define table structure\n",
    "        table_name = 'meta-data'\n",
    "        topics_table = Table(\n",
    "            table_name,\n",
    "            metadata,\n",
    "            Column('Filename', String),\n",
    "            Column('Time', String),\n",
    "            Column('MD5 Identifier', String),\n",
    "            Column('Encoding Version', String),\n",
    "            Column('Lang', String),\n",
    "            Column('Application Identifier', String),\n",
    "            Column('Application Description', String),\n",
    "            Column('Application Version', String),\n",
    "            Column('Application Reference URL', String),\n",
    "            Column('S3 URL', String) \n",
    "        )\n",
    "    \n",
    "        # create or replace table in Snowflake\n",
    "        topics_table.drop(engine, checkfirst=True)  # Drop table if exists\n",
    "        topics_table.create(engine)\n",
    "        print('Table created')\n",
    "        print('-------Ending Snowflake table creation-------')\n",
    "        return table_name\n",
    "    except:\n",
    "        print(\"Error creating Snowflake Table\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f774d4c7-992a-4f80-9ee4-18c752aa7a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uploadDataToSnowflake(engine, table_name):\n",
    "    file_format_name = 'meta_file_format'\n",
    "    field_delimiter = '\\t'\n",
    "    skip_header = 1\n",
    "    skip_blank_lines = True\n",
    "    trim_space = True\n",
    "    field_optionally_enclosed_by = None\n",
    "\n",
    "    file_path = \"../../data/extracted-pdf-data_Grobid/grobid_metadata_new.csv\"\n",
    "    stage_name = \"metadata_csv_stage\"\n",
    "    \n",
    "    # Create or replace file format\n",
    "    create_file_format_sql = f\"\"\"\n",
    "    CREATE OR REPLACE FILE FORMAT {file_format_name}\n",
    "    TYPE = 'CSV'\n",
    "    FIELD_DELIMITER = '{field_delimiter}'\n",
    "    SKIP_HEADER = {skip_header}\n",
    "    SKIP_BLANK_LINES = {skip_blank_lines}\n",
    "    TRIM_SPACE = {trim_space}\n",
    "    \"\"\"\n",
    "\n",
    "    # create or replace Stage\n",
    "    create_stage = f\"\"\"CREATE OR REPLACE STAGE {stage_name} DIRECTORY = ( ENABLE = true );\"\"\"\n",
    "    \n",
    "    # Put file format\n",
    "    put_command = f\"\"\"PUT 'file://{file_path}' @{stage_name}\"\"\"\n",
    "\n",
    "    # Copy to table\n",
    "    copy_sql = f\"\"\"\n",
    "        COPY INTO \"{table_name}\" FROM '@{stage_name}'\n",
    "        FILE_FORMAT = (FORMAT_NAME = {file_format_name})\n",
    "        \"\"\"\n",
    "\n",
    "    try:\n",
    "        print('-------Starting Data Upload to Snowflake-------')\n",
    "        \n",
    "        with engine.connect() as connection: \n",
    "            # execute file format\n",
    "            connection.execute(create_file_format_sql)\n",
    "            print('File Format created')\n",
    "            \n",
    "            # execute stage creation\n",
    "            connection .execute(create_stage)\n",
    "            print('Stage created')\n",
    "            \n",
    "            # put file in stage\n",
    "            connection.execute(put_command)\n",
    "            print('Put file into stage')\n",
    "\n",
    "            # put file in stage\n",
    "            connection.execute(copy_sql)\n",
    "            print('Copied file into table')\n",
    "\n",
    "        print('-------Ending Data Upload to Snowflake-------')\n",
    "    except: \n",
    "        print(\"Error creating Uploading to Snowflake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6d128d3e-a29c-44f7-b37e-f9d6388086da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload MetaData to Snowflake\n",
    "def upload_metadata():\n",
    "    snowflake_database=\"ASSIGNMENT_2\"\n",
    "    snowflake_schema=\"RR_SCHEMA\"\n",
    "    snowflake_warehouse=\"WH_2\"\n",
    "    engine = getSnowflakeEngine(snowflake_database, snowflake_schema, snowflake_warehouse)\n",
    "    \n",
    "    # create table \n",
    "    table_name = createMetadataTable(engine)\n",
    "\n",
    "    # uploading data to snowflake\n",
    "    uploadDataToSnowflake(engine, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "108b4653-3a28-4877-975f-3a563599d6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Starting Snowflake table creation-------\n",
      "Table created\n",
      "-------Ending Snowflake table creation-------\n",
      "-------Starting Data Upload to Snowflake-------\n",
      "File Format created\n",
      "Stage created\n",
      "Put file into stage\n",
      "Copied file into table\n",
      "-------Ending Data Upload to Snowflake-------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # upload data to S3\n",
    "    upload_scrape_file()\n",
    "    \n",
    "    # upload extractes pdf data to S3 and get urls \n",
    "    s3_urls = upload_extracted_data_files()\n",
    "\n",
    "    # update metadata csv\n",
    "    UpdateCSV(s3_urls)\n",
    "\n",
    "    # upload metadata\n",
    "    upload_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae9f29b-4da0-4172-91d6-3adcfd82675f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
